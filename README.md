# moreformers
Experimenting on a bunch of transformer variants I come up with. They vary in attention mechanisms, block configurations, etc. Vanilla GPT and LLaMa model definitions (in pytorch) are included as baseline

